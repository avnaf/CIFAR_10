# ============================================================================================================================================= #
#Imports
import cv2
import numpy as np
import time
from collections import Counter
# ============================================================================================================================================= #

# ============================================================================================================================================= #
#Globals:
batch_dir = r'C:\Users\Naftalis\Desktop\Computer Vision\Project\cifar-10-batches-py'
data_1 = batch_dir + '\\' + r'data_batch_1'
data_2 = batch_dir + '\\' + r'data_batch_2'
data_3 = batch_dir + '\\' + r'data_batch_3'
data_4 = batch_dir + '\\' + r'data_batch_4'
data_5 = batch_dir + '\\' + r'data_batch_5'
data_batch = [data_1, data_2, data_3, data_4, data_5]
test = batch_dir + '\\' + r'test_batch'
predictions = []
actuals = []
#x_val = [2*i+1 for i in range(2)]
#y_val = [2*i+1 for i in range(2)]
#size_val = [i/2 for i in range(4)]
#angle_val = [i for i in range(-90,90,90)]
#print 'starting kp...'
#g_key_points = [cv2.KeyPoint(x,y,size,angle) for x in x_val for y in y_val for size in size_val for angle in angle_val]
#print 'num of kp: {}'.format(len(g_key_points))
#print 'finished kp...'
# ============================================================================================================================================= #

# ============================================================================================================================================= #
# dict = {data : lables}
# data = 10000*3072 numpy array of uint8
#   Each row of the array stores a 32x32 colour image
#   The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue
#   The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image
# lables =  list of 10000 numbers in the range 0-9
#   The number at index i indicates the label of the ith image in the array data
# ============================================================================================================================================= #
# The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:
#   label_names = 10-element list which gives meaningful names to the numeric labels in the labels array described above.
#   For example, label_names[0] == "airplane", label_names[1] == "automobile", etc.
# ============================================================================================================================================= #
def unpickle(file, print_msg = False):
    if print_msg:
        print 'starting unpickling {}...'.format(file[file.rfind('\\')+1:])
        
    import cPickle
    with open(file, 'rb') as fo:
        dict = cPickle.load(fo)
        
    if print_msg:
        print 'finished unpickling {}...'.format(file[file.rfind('\\')+1:])
        print '#'*50 
    return dict

# ============================================================================================================================================= #
# input: array of 3072 elements
# ============================================================================================================================================= #
def img_2_RGB(im, print_msg = False):
    if print_msg:
        print 'reshaping image to RGB format (32*32*3)'
        
    mat = np.zeros((32,32,3), np.uint8)
    for i in range(32*32*3):
        mat[(i % (32*32)) // 32][i % 32][i // (32*32)] = im[i]
    return mat

# ============================================================================================================================================= #
# input: image list, heading (optional), delay [mili-seconds] (optional)
# ============================================================================================================================================= #
def show_img(i, heading = '', delay = 1000):
    cv2.imshow(heading, i) 
    cv2.waitKey(delay)
    cv2.destroyAllWindows()
# ============================================================================================================================================= #
# 
# ============================================================================================================================================= #
def find_good_kp(num_of_test_images = 100, hueristic = 'good image for every class', num_of_images_to_sum = 4):
    #print num_of_test_images
    tst = unpickle(test)
    sift = cv2.xfeatures2d.SIFT_create()
    tst_images = tst.values()[0][:num_of_test_images]
    good_kp = []
    
    if hueristic == 'richest image':
        max_kp = 0
        for im in enumerate(tst_images):
            im = img_2_RGB(im[1])
            kp = sift.detect(im, None)
            #print kp, len(kp)
            if len(kp) > max_kp:
                max_kp = len(kp)
                good_kp = kp
    elif hueristic == 'several images':  
        for im in enumerate(tst_images):
            if im[0] == num_of_images_to_sum:
                break
            im = img_2_RGB(im[1])
            kp = sift.detect(im, None)
            good_kp += kp
    elif hueristic == 'good image for every class':
        tst_labels = tst.values()[1][:num_of_test_images]
        used_labels = []
        for pair in enumerate(zip(tst_images, tst_labels)):
            im = img_2_RGB(pair[1][0])
            lb = pair[1][1]
            if lb not in used_labels and len(sift.detect(im, None)) >= 20:
                good_kp += sift.detect(im, None)
            if len(used_labels) == 10:
                break
        
        
        
    del tst_images
    del sift
    print '# of descriptors {}'.format(len(good_kp))
    return good_kp
# ============================================================================================================================================= #
# Done only once
# ============================================================================================================================================= #
tst = unpickle(test,True)    
tst_size = 10000
tst_images = tst.values()[0][:tst_size]
tst_labels = tst.values()[1][:tst_size] 
  
tst_relevant_sized_descriptors = []
tst_relevant_labels = [] 
hopefully_good_kp = find_good_kp()

for im,lb in zip(tst_images, tst_labels):
    im = img_2_RGB(im)
    
    #old_kp = sift.detect(im, None)
    sift = cv2.xfeatures2d.SIFT_create()
    kp,ds = sift.compute(im, hopefully_good_kp)
    
    tst_relevant_sized_descriptors.append(ds.flatten())
    tst_relevant_labels.append(lb)
        
tst_X = np.array(tst_relevant_sized_descriptors)
test_size = len(tst_X)
# ============================================================================================================================================= #

def my_main(num_of_images_to_check, offset):
    
    global hopefully_good_kp
    global test_size
    global tst_X
    global tst_relevant_labels
    
    start = time.time()
    print '#'*100 
    sift = cv2.xfeatures2d.SIFT_create()
    print 'sift created...'
    
    data_set_counter = 0
    
    relevant_sized_descriptors = []
    relevant_labels = []
    
    #print 'using {} descriptors'.format(num_of_des)
    for data in data_batch:
        data_set_counter += 1
        d = unpickle(data,True)

        images = d.values()[0][offset:offset + num_of_images_to_check]
        labels = d.values()[1][offset:offset + num_of_images_to_check]
        
        print 'starting computing training {} descriptors...'.format(data_set_counter)
        for im,lb in zip(images, labels):
            im = img_2_RGB(im)
            #old_kp = sift.detect(im, None)
            kp,ds = sift.compute(im, hopefully_good_kp)
            #if ds is not None and len(ds) == num_of_des:
            relevant_sized_descriptors.append(ds.flatten())
            relevant_labels.append(lb)
        print 'finished computing training {} descriptors...'.format(data_set_counter)
        print '#'*50 

    X = np.array(relevant_sized_descriptors)
    training_size = len(X)
    y = np.array(relevant_labels)
    
    from sklearn.svm import SVC 
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    
    clf = SVC(C = 0.001, kernel='poly', degree = 3, gamma = 0.001, probability=True)#best so far
    clfs = []
    #clfs += [LinearDiscriminantAnalysis()]                                                      #1
    clfs += [LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 'auto')]                   #2
#    clfs += [LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 0.2)]                      #3
#    clfs += [LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 0.5)]                      #4
#    clfs += [LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 0.8)]                      #5
#    clfs += [LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 1)]                        #6
#    clfs += [LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 'auto')]                  #7
#    clfs += [LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 0.2)]                     #8
#    clfs += [LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 0.5)]                     #9
#    clfs += [LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 0.8)]                     #10
#    clfs += [LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 1)]                       #11

    

    
    fit_start_time = time.time()
    print 'starting fitting...' 
    for clf in enumerate(clfs):
        each_fit_start_time = time.time()
        print 'start fit'
        clf[1].fit(X,y)
        print 'end   fit ({} seconds)'.format(time.time() - each_fit_start_time) 
    print 'finished fitting... ({} seconds)'.format(time.time() - fit_start_time)
    print '#'*50

    global predictions
    global actuals
    for clf in enumerate(clfs):
        score_time = time.time()
        print 'kernel # {}: training_size = {}, test_size = {}... Rate = {}% ({} seconds)'.format(clf[0]+1,training_size,test_size, clf[1].score(tst_X,tst_relevant_labels), time.time() - score_time)
        predictions.append(clf[1].predict(tst_X).flatten())
        # just for the 1st time. It doesn't change
        if not offset:
            actuals = relevant_labels
            
    
        
    #clean-up
    del relevant_sized_descriptors
    del relevant_labels
    del X
    del y
    del clf
    del d
    del images
    del labels
    del sift

    end = time.time()
    print 'time elapsed: {}'.format(end - start)
    return end-start


    
size = 10000
for i in range(1):
    print '#'
    my_main(size,size*i)
